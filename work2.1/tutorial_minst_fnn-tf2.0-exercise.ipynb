{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    #normalize\n",
    "    x = x/255.0\n",
    "    x_test = x_test/255.0\n",
    "    return (x, y), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([1, 2, 3, 4], ['a', 'b', 'c', 'd'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel:\n",
    "    def __init__(self):\n",
    "        ####################\n",
    "        '''声明模型对应的参数'''\n",
    "        self.W1 = tf.Variable(tf.random.normal(shape=(28 * 28, 512), stddev=0.05), trainable=True)\n",
    "        self.b1 = tf.Variable(tf.zeros(shape=(512,)), trainable=True)\n",
    "        self.W2 = tf.Variable(tf.random.normal(shape=(512, 10), stddev=0.05), trainable=True)\n",
    "        self.b2 = tf.Variable(tf.zeros(shape=(10,)), trainable=True)\n",
    "        ####################\n",
    "    def __call__(self, x):\n",
    "        ####################\n",
    "        '''实现模型函数体，返回未归一化的logits'''\n",
    "        x_flattened = tf.reshape(x, [-1, 784])\n",
    "        h1 = tf.matmul(x_flattened, self.W1) + self.b1\n",
    "        activated_h1 = tf.tanh(h1)\n",
    "        logits = tf.matmul(activated_h1, self.W2) + self.b2\n",
    "        ####################\n",
    "        return logits\n",
    "        \n",
    "model = myModel()\n",
    "\n",
    "optimizer = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels))\n",
    "\n",
    "@tf.function\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "\n",
    "    # compute gradient\n",
    "    trainable_vars = [model.W1, model.W2, model.b1, model.b2]\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    for g, v in zip(grads, trainable_vars):\n",
    "        v.assign_sub(0.01*g)\n",
    "\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "\n",
    "    # loss and accuracy is scalar tensor\n",
    "    return loss, accuracy\n",
    "\n",
    "@tf.function\n",
    "def test(model, x, y):\n",
    "    logits = model(x)\n",
    "    loss = compute_loss(logits, y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 2.393965 ; accuracy 0.061983332\n",
      "epoch 1 : loss 2.3701549 ; accuracy 0.0687\n",
      "epoch 2 : loss 2.347191 ; accuracy 0.076516666\n",
      "epoch 3 : loss 2.324986 ; accuracy 0.08528333\n",
      "epoch 4 : loss 2.3034635 ; accuracy 0.09463333\n",
      "epoch 5 : loss 2.2825584 ; accuracy 0.10408334\n",
      "epoch 6 : loss 2.262213 ; accuracy 0.11586667\n",
      "epoch 7 : loss 2.242379 ; accuracy 0.12791666\n",
      "epoch 8 : loss 2.2230124 ; accuracy 0.14101666\n",
      "epoch 9 : loss 2.2040763 ; accuracy 0.15543333\n",
      "epoch 10 : loss 2.185538 ; accuracy 0.17115\n",
      "epoch 11 : loss 2.1673691 ; accuracy 0.18895\n",
      "epoch 12 : loss 2.1495447 ; accuracy 0.2077\n",
      "epoch 13 : loss 2.1320431 ; accuracy 0.22695\n",
      "epoch 14 : loss 2.1148455 ; accuracy 0.2476\n",
      "epoch 15 : loss 2.097935 ; accuracy 0.26905\n",
      "epoch 16 : loss 2.0812972 ; accuracy 0.2908\n",
      "epoch 17 : loss 2.0649195 ; accuracy 0.31238332\n",
      "epoch 18 : loss 2.0487907 ; accuracy 0.33338332\n",
      "epoch 19 : loss 2.0329006 ; accuracy 0.3532\n",
      "epoch 20 : loss 2.0172405 ; accuracy 0.37326667\n",
      "epoch 21 : loss 2.0018032 ; accuracy 0.39011666\n",
      "epoch 22 : loss 1.986581 ; accuracy 0.4078\n",
      "epoch 23 : loss 1.9715685 ; accuracy 0.42293334\n",
      "epoch 24 : loss 1.9567597 ; accuracy 0.43783334\n",
      "epoch 25 : loss 1.9421498 ; accuracy 0.4518\n",
      "epoch 26 : loss 1.9277344 ; accuracy 0.46485\n",
      "epoch 27 : loss 1.9135092 ; accuracy 0.47793335\n",
      "epoch 28 : loss 1.8994708 ; accuracy 0.48936668\n",
      "epoch 29 : loss 1.8856155 ; accuracy 0.50045\n",
      "epoch 30 : loss 1.8719401 ; accuracy 0.51023334\n",
      "epoch 31 : loss 1.858442 ; accuracy 0.5200667\n",
      "epoch 32 : loss 1.8451177 ; accuracy 0.52961665\n",
      "epoch 33 : loss 1.8319651 ; accuracy 0.5380667\n",
      "epoch 34 : loss 1.8189816 ; accuracy 0.54651666\n",
      "epoch 35 : loss 1.8061646 ; accuracy 0.55488336\n",
      "epoch 36 : loss 1.7935121 ; accuracy 0.5624\n",
      "epoch 37 : loss 1.7810216 ; accuracy 0.56921667\n",
      "epoch 38 : loss 1.768691 ; accuracy 0.5764667\n",
      "epoch 39 : loss 1.7565182 ; accuracy 0.5830167\n",
      "epoch 40 : loss 1.7445011 ; accuracy 0.58973336\n",
      "epoch 41 : loss 1.7326379 ; accuracy 0.59541667\n",
      "epoch 42 : loss 1.7209263 ; accuracy 0.60085\n",
      "epoch 43 : loss 1.7093645 ; accuracy 0.60611665\n",
      "epoch 44 : loss 1.6979508 ; accuracy 0.61158335\n",
      "epoch 45 : loss 1.6866828 ; accuracy 0.6164167\n",
      "epoch 46 : loss 1.6755588 ; accuracy 0.6218167\n",
      "epoch 47 : loss 1.6645774 ; accuracy 0.6260333\n",
      "epoch 48 : loss 1.653736 ; accuracy 0.6300167\n",
      "epoch 49 : loss 1.643033 ; accuracy 0.63463336\n",
      "epoch 50 : loss 1.6324669 ; accuracy 0.6386833\n",
      "epoch 51 : loss 1.6220356 ; accuracy 0.64288336\n",
      "epoch 52 : loss 1.6117375 ; accuracy 0.6462\n",
      "epoch 53 : loss 1.6015707 ; accuracy 0.65043336\n",
      "epoch 54 : loss 1.5915333 ; accuracy 0.6545\n",
      "epoch 55 : loss 1.5816237 ; accuracy 0.658\n",
      "epoch 56 : loss 1.5718403 ; accuracy 0.6616\n",
      "epoch 57 : loss 1.562181 ; accuracy 0.66513336\n",
      "epoch 58 : loss 1.5526443 ; accuracy 0.6677667\n",
      "epoch 59 : loss 1.5432284 ; accuracy 0.6707\n",
      "epoch 60 : loss 1.5339317 ; accuracy 0.6734167\n",
      "epoch 61 : loss 1.5247527 ; accuracy 0.67628336\n",
      "epoch 62 : loss 1.5156896 ; accuracy 0.67905\n",
      "epoch 63 : loss 1.5067405 ; accuracy 0.682\n",
      "epoch 64 : loss 1.4979042 ; accuracy 0.68455\n",
      "epoch 65 : loss 1.4891787 ; accuracy 0.6869\n",
      "epoch 66 : loss 1.4805627 ; accuracy 0.68918335\n",
      "epoch 67 : loss 1.4720545 ; accuracy 0.69151664\n",
      "epoch 68 : loss 1.4636525 ; accuracy 0.69381666\n",
      "epoch 69 : loss 1.4553552 ; accuracy 0.69621664\n",
      "epoch 70 : loss 1.4471611 ; accuracy 0.6985833\n",
      "epoch 71 : loss 1.4390688 ; accuracy 0.70063335\n",
      "epoch 72 : loss 1.4310763 ; accuracy 0.70245\n",
      "epoch 73 : loss 1.4231828 ; accuracy 0.7043167\n",
      "epoch 74 : loss 1.4153866 ; accuracy 0.70638335\n",
      "epoch 75 : loss 1.4076862 ; accuracy 0.70846665\n",
      "epoch 76 : loss 1.4000801 ; accuracy 0.71096665\n",
      "epoch 77 : loss 1.392567 ; accuracy 0.7131\n",
      "epoch 78 : loss 1.3851455 ; accuracy 0.7148833\n",
      "epoch 79 : loss 1.3778144 ; accuracy 0.7166833\n",
      "epoch 80 : loss 1.3705721 ; accuracy 0.7182\n",
      "epoch 81 : loss 1.3634175 ; accuracy 0.71991664\n",
      "epoch 82 : loss 1.3563491 ; accuracy 0.72176665\n",
      "epoch 83 : loss 1.3493656 ; accuracy 0.72386664\n",
      "epoch 84 : loss 1.3424659 ; accuracy 0.7256\n",
      "epoch 85 : loss 1.3356487 ; accuracy 0.72718334\n",
      "epoch 86 : loss 1.3289126 ; accuracy 0.7289\n",
      "epoch 87 : loss 1.3222566 ; accuracy 0.7304\n",
      "epoch 88 : loss 1.3156794 ; accuracy 0.73185\n",
      "epoch 89 : loss 1.3091797 ; accuracy 0.73361665\n",
      "epoch 90 : loss 1.3027568 ; accuracy 0.7348833\n",
      "epoch 91 : loss 1.296409 ; accuracy 0.73618335\n",
      "epoch 92 : loss 1.2901354 ; accuracy 0.73751664\n",
      "epoch 93 : loss 1.283935 ; accuracy 0.73906666\n",
      "epoch 94 : loss 1.2778065 ; accuracy 0.74045\n",
      "epoch 95 : loss 1.2717489 ; accuracy 0.7416667\n",
      "epoch 96 : loss 1.265761 ; accuracy 0.7427667\n",
      "epoch 97 : loss 1.2598422 ; accuracy 0.74365\n",
      "epoch 98 : loss 1.2539911 ; accuracy 0.7450333\n",
      "epoch 99 : loss 1.2482067 ; accuracy 0.74633336\n",
      "epoch 100 : loss 1.2424881 ; accuracy 0.74736667\n",
      "epoch 101 : loss 1.2368344 ; accuracy 0.7485167\n",
      "epoch 102 : loss 1.2312446 ; accuracy 0.74976665\n",
      "epoch 103 : loss 1.2257174 ; accuracy 0.75088334\n",
      "epoch 104 : loss 1.2202525 ; accuracy 0.75226665\n",
      "epoch 105 : loss 1.2148483 ; accuracy 0.75336665\n",
      "epoch 106 : loss 1.2095045 ; accuracy 0.7543333\n",
      "epoch 107 : loss 1.20422 ; accuracy 0.75561666\n",
      "epoch 108 : loss 1.1989938 ; accuracy 0.7568167\n",
      "epoch 109 : loss 1.193825 ; accuracy 0.75808334\n",
      "epoch 110 : loss 1.1887131 ; accuracy 0.759\n",
      "epoch 111 : loss 1.1836568 ; accuracy 0.76026666\n",
      "epoch 112 : loss 1.1786557 ; accuracy 0.76136667\n",
      "epoch 113 : loss 1.1737088 ; accuracy 0.7625167\n",
      "epoch 114 : loss 1.1688154 ; accuracy 0.7635\n",
      "epoch 115 : loss 1.1639745 ; accuracy 0.76453334\n",
      "epoch 116 : loss 1.1591855 ; accuracy 0.7655333\n",
      "epoch 117 : loss 1.1544479 ; accuracy 0.76643336\n",
      "epoch 118 : loss 1.1497606 ; accuracy 0.7673333\n",
      "epoch 119 : loss 1.1451228 ; accuracy 0.7682\n",
      "epoch 120 : loss 1.1405342 ; accuracy 0.76893336\n",
      "epoch 121 : loss 1.1359937 ; accuracy 0.7697333\n",
      "epoch 122 : loss 1.1315008 ; accuracy 0.7705333\n",
      "epoch 123 : loss 1.1270547 ; accuracy 0.7712\n",
      "epoch 124 : loss 1.1226549 ; accuracy 0.77213335\n",
      "epoch 125 : loss 1.1183009 ; accuracy 0.77311665\n",
      "epoch 126 : loss 1.1139914 ; accuracy 0.77396667\n",
      "epoch 127 : loss 1.1097265 ; accuracy 0.77463335\n",
      "epoch 128 : loss 1.1055052 ; accuracy 0.7754167\n",
      "epoch 129 : loss 1.1013268 ; accuracy 0.7762333\n",
      "epoch 130 : loss 1.097191 ; accuracy 0.7769333\n",
      "epoch 131 : loss 1.0930969 ; accuracy 0.77785\n",
      "epoch 132 : loss 1.0890442 ; accuracy 0.77848333\n",
      "epoch 133 : loss 1.0850321 ; accuracy 0.7791167\n",
      "epoch 134 : loss 1.0810602 ; accuracy 0.7798667\n",
      "epoch 135 : loss 1.0771277 ; accuracy 0.7804\n",
      "epoch 136 : loss 1.0732343 ; accuracy 0.7813333\n",
      "epoch 137 : loss 1.0693793 ; accuracy 0.78205\n",
      "epoch 138 : loss 1.0655625 ; accuracy 0.7827333\n",
      "epoch 139 : loss 1.0617828 ; accuracy 0.78328335\n",
      "epoch 140 : loss 1.0580401 ; accuracy 0.7841167\n",
      "epoch 141 : loss 1.0543339 ; accuracy 0.7847667\n",
      "epoch 142 : loss 1.0506637 ; accuracy 0.7854\n",
      "epoch 143 : loss 1.0470288 ; accuracy 0.78583336\n",
      "epoch 144 : loss 1.0434288 ; accuracy 0.7866167\n",
      "epoch 145 : loss 1.0398632 ; accuracy 0.787\n",
      "epoch 146 : loss 1.0363319 ; accuracy 0.7877\n",
      "epoch 147 : loss 1.0328339 ; accuracy 0.78831667\n",
      "epoch 148 : loss 1.0293691 ; accuracy 0.7892333\n",
      "epoch 149 : loss 1.025937 ; accuracy 0.78995\n",
      "epoch 150 : loss 1.022537 ; accuracy 0.79043335\n",
      "epoch 151 : loss 1.0191689 ; accuracy 0.7909333\n",
      "epoch 152 : loss 1.0158322 ; accuracy 0.7917\n",
      "epoch 153 : loss 1.0125264 ; accuracy 0.7923667\n",
      "epoch 154 : loss 1.0092512 ; accuracy 0.793\n",
      "epoch 155 : loss 1.0060062 ; accuracy 0.7935167\n",
      "epoch 156 : loss 1.0027909 ; accuracy 0.79391664\n",
      "epoch 157 : loss 0.999605 ; accuracy 0.7945333\n",
      "epoch 158 : loss 0.99644816 ; accuracy 0.7949167\n",
      "epoch 159 : loss 0.99331987 ; accuracy 0.7953167\n",
      "epoch 160 : loss 0.9902198 ; accuracy 0.79588336\n",
      "epoch 161 : loss 0.9871476 ; accuracy 0.7964333\n",
      "epoch 162 : loss 0.9841031 ; accuracy 0.7970667\n",
      "epoch 163 : loss 0.98108566 ; accuracy 0.7973833\n",
      "epoch 164 : loss 0.97809505 ; accuracy 0.7979\n",
      "epoch 165 : loss 0.97513086 ; accuracy 0.79836667\n",
      "epoch 166 : loss 0.97219294 ; accuracy 0.7988667\n",
      "epoch 167 : loss 0.96928084 ; accuracy 0.7993\n",
      "epoch 168 : loss 0.9663941 ; accuracy 0.79971665\n",
      "epoch 169 : loss 0.9635328 ; accuracy 0.80016667\n",
      "epoch 170 : loss 0.96069616 ; accuracy 0.8005667\n",
      "epoch 171 : loss 0.95788413 ; accuracy 0.8009833\n",
      "epoch 172 : loss 0.95509636 ; accuracy 0.8015\n",
      "epoch 173 : loss 0.9523325 ; accuracy 0.8017667\n",
      "epoch 174 : loss 0.9495923 ; accuracy 0.8024833\n",
      "epoch 175 : loss 0.94687545 ; accuracy 0.8030667\n",
      "epoch 176 : loss 0.9441818 ; accuracy 0.8034833\n",
      "epoch 177 : loss 0.94151074 ; accuracy 0.8038833\n",
      "epoch 178 : loss 0.93886226 ; accuracy 0.8043333\n",
      "epoch 179 : loss 0.9362361 ; accuracy 0.8046167\n",
      "epoch 180 : loss 0.93363184 ; accuracy 0.8049833\n",
      "epoch 181 : loss 0.9310493 ; accuracy 0.8053333\n",
      "epoch 182 : loss 0.92848814 ; accuracy 0.80565\n",
      "epoch 183 : loss 0.9259482 ; accuracy 0.8059667\n",
      "epoch 184 : loss 0.9234292 ; accuracy 0.8065\n",
      "epoch 185 : loss 0.92093086 ; accuracy 0.80691665\n",
      "epoch 186 : loss 0.9184529 ; accuracy 0.80721664\n",
      "epoch 187 : loss 0.91599524 ; accuracy 0.8078833\n",
      "epoch 188 : loss 0.91355747 ; accuracy 0.80843335\n",
      "epoch 189 : loss 0.9111394 ; accuracy 0.80878335\n",
      "epoch 190 : loss 0.9087409 ; accuracy 0.8092167\n",
      "epoch 191 : loss 0.9063616 ; accuracy 0.8096\n",
      "epoch 192 : loss 0.9040013 ; accuracy 0.8099667\n",
      "epoch 193 : loss 0.90165985 ; accuracy 0.81028336\n",
      "epoch 194 : loss 0.89933693 ; accuracy 0.8107167\n",
      "epoch 195 : loss 0.8970324 ; accuracy 0.8110667\n",
      "epoch 196 : loss 0.894746 ; accuracy 0.8113\n",
      "epoch 197 : loss 0.89247763 ; accuracy 0.81166667\n",
      "epoch 198 : loss 0.89022696 ; accuracy 0.8121167\n",
      "epoch 199 : loss 0.8879939 ; accuracy 0.8124167\n",
      "epoch 200 : loss 0.88577807 ; accuracy 0.81263334\n",
      "epoch 201 : loss 0.88357943 ; accuracy 0.8129167\n",
      "epoch 202 : loss 0.8813978 ; accuracy 0.8132833\n",
      "epoch 203 : loss 0.8792328 ; accuracy 0.81365\n",
      "epoch 204 : loss 0.8770845 ; accuracy 0.81401664\n",
      "epoch 205 : loss 0.87495255 ; accuracy 0.81441665\n",
      "epoch 206 : loss 0.8728367 ; accuracy 0.8147333\n",
      "epoch 207 : loss 0.87073696 ; accuracy 0.81495\n",
      "epoch 208 : loss 0.86865306 ; accuracy 0.81516665\n",
      "epoch 209 : loss 0.86658484 ; accuracy 0.8156\n",
      "epoch 210 : loss 0.86453205 ; accuracy 0.8160333\n",
      "epoch 211 : loss 0.86249465 ; accuracy 0.8164833\n",
      "epoch 212 : loss 0.8604724 ; accuracy 0.8167167\n",
      "epoch 213 : loss 0.8584651 ; accuracy 0.81695\n",
      "epoch 214 : loss 0.8564727 ; accuracy 0.81731665\n",
      "epoch 215 : loss 0.8544948 ; accuracy 0.81765\n",
      "epoch 216 : loss 0.8525315 ; accuracy 0.81803334\n",
      "epoch 217 : loss 0.8505826 ; accuracy 0.81853336\n",
      "epoch 218 : loss 0.84864783 ; accuracy 0.8189667\n",
      "epoch 219 : loss 0.8467271 ; accuracy 0.8195\n",
      "epoch 220 : loss 0.84482026 ; accuracy 0.81976664\n",
      "epoch 221 : loss 0.8429272 ; accuracy 0.8200833\n",
      "epoch 222 : loss 0.84104764 ; accuracy 0.8203667\n",
      "epoch 223 : loss 0.83918166 ; accuracy 0.8207333\n",
      "epoch 224 : loss 0.8373289 ; accuracy 0.8210667\n",
      "epoch 225 : loss 0.83548933 ; accuracy 0.82126665\n",
      "epoch 226 : loss 0.8336628 ; accuracy 0.8215167\n",
      "epoch 227 : loss 0.8318491 ; accuracy 0.8218\n",
      "epoch 228 : loss 0.83004826 ; accuracy 0.8221\n",
      "epoch 229 : loss 0.8282599 ; accuracy 0.8224\n",
      "epoch 230 : loss 0.82648414 ; accuracy 0.82268333\n",
      "epoch 231 : loss 0.8247207 ; accuracy 0.823\n",
      "epoch 232 : loss 0.82296944 ; accuracy 0.8233167\n",
      "epoch 233 : loss 0.8212305 ; accuracy 0.82365\n",
      "epoch 234 : loss 0.81950337 ; accuracy 0.82381666\n",
      "epoch 235 : loss 0.81778806 ; accuracy 0.82411665\n",
      "epoch 236 : loss 0.8160846 ; accuracy 0.8243167\n",
      "epoch 237 : loss 0.8143927 ; accuracy 0.82458335\n",
      "epoch 238 : loss 0.8127124 ; accuracy 0.82485\n",
      "epoch 239 : loss 0.8110434 ; accuracy 0.82516664\n",
      "epoch 240 : loss 0.80938566 ; accuracy 0.82545\n",
      "epoch 241 : loss 0.8077391 ; accuracy 0.82575\n",
      "epoch 242 : loss 0.80610365 ; accuracy 0.8260667\n",
      "epoch 243 : loss 0.80447894 ; accuracy 0.82628334\n",
      "epoch 244 : loss 0.8028652 ; accuracy 0.8264833\n",
      "epoch 245 : loss 0.8012622 ; accuracy 0.8268333\n",
      "epoch 246 : loss 0.7996698 ; accuracy 0.82705\n",
      "epoch 247 : loss 0.7980879 ; accuracy 0.82731664\n",
      "epoch 248 : loss 0.79651636 ; accuracy 0.8275\n",
      "epoch 249 : loss 0.7949552 ; accuracy 0.82776666\n",
      "epoch 250 : loss 0.79340416 ; accuracy 0.82795\n",
      "epoch 251 : loss 0.7918632 ; accuracy 0.82815\n",
      "epoch 252 : loss 0.7903323 ; accuracy 0.82841665\n",
      "epoch 253 : loss 0.78881127 ; accuracy 0.8287167\n",
      "epoch 254 : loss 0.78730005 ; accuracy 0.8289\n",
      "epoch 255 : loss 0.7857986 ; accuracy 0.8292\n",
      "epoch 256 : loss 0.78430676 ; accuracy 0.82948333\n",
      "epoch 257 : loss 0.78282446 ; accuracy 0.8297833\n",
      "epoch 258 : loss 0.78135157 ; accuracy 0.83\n",
      "epoch 259 : loss 0.779888 ; accuracy 0.83015\n",
      "epoch 260 : loss 0.77843374 ; accuracy 0.83035\n",
      "epoch 261 : loss 0.77698857 ; accuracy 0.83055\n",
      "epoch 262 : loss 0.7755525 ; accuracy 0.83073336\n",
      "epoch 263 : loss 0.7741255 ; accuracy 0.8308\n",
      "epoch 264 : loss 0.77270734 ; accuracy 0.83101666\n",
      "epoch 265 : loss 0.77129805 ; accuracy 0.8311833\n",
      "epoch 266 : loss 0.7698975 ; accuracy 0.83143336\n",
      "epoch 267 : loss 0.7685056 ; accuracy 0.83168334\n",
      "epoch 268 : loss 0.7671223 ; accuracy 0.83185\n",
      "epoch 269 : loss 0.76574755 ; accuracy 0.83196664\n",
      "epoch 270 : loss 0.7643812 ; accuracy 0.83206666\n",
      "epoch 271 : loss 0.7630232 ; accuracy 0.8322333\n",
      "epoch 272 : loss 0.76167345 ; accuracy 0.8325167\n",
      "epoch 273 : loss 0.7603319 ; accuracy 0.83276665\n",
      "epoch 274 : loss 0.75899845 ; accuracy 0.8329\n",
      "epoch 275 : loss 0.757673 ; accuracy 0.8330333\n",
      "epoch 276 : loss 0.7563556 ; accuracy 0.83323336\n",
      "epoch 277 : loss 0.7550461 ; accuracy 0.83346665\n",
      "epoch 278 : loss 0.7537445 ; accuracy 0.83381665\n",
      "epoch 279 : loss 0.7524506 ; accuracy 0.8339667\n",
      "epoch 280 : loss 0.7511643 ; accuracy 0.83405\n",
      "epoch 281 : loss 0.7498857 ; accuracy 0.83423334\n",
      "epoch 282 : loss 0.74861467 ; accuracy 0.83456665\n",
      "epoch 283 : loss 0.74735105 ; accuracy 0.8347\n",
      "epoch 284 : loss 0.74609494 ; accuracy 0.83496666\n",
      "epoch 285 : loss 0.74484617 ; accuracy 0.8351833\n",
      "epoch 286 : loss 0.74360466 ; accuracy 0.8353\n",
      "epoch 287 : loss 0.7423703 ; accuracy 0.8355833\n",
      "epoch 288 : loss 0.7411433 ; accuracy 0.83563334\n",
      "epoch 289 : loss 0.73992324 ; accuracy 0.8358667\n",
      "epoch 290 : loss 0.73871017 ; accuracy 0.83603334\n",
      "epoch 291 : loss 0.7375042 ; accuracy 0.8361667\n",
      "epoch 292 : loss 0.73630506 ; accuracy 0.8363\n",
      "epoch 293 : loss 0.7351129 ; accuracy 0.8365\n",
      "epoch 294 : loss 0.7339274 ; accuracy 0.83655\n",
      "epoch 295 : loss 0.7327487 ; accuracy 0.83671665\n",
      "epoch 296 : loss 0.73157656 ; accuracy 0.8369833\n",
      "epoch 297 : loss 0.7304112 ; accuracy 0.83715\n",
      "epoch 298 : loss 0.72925234 ; accuracy 0.8372167\n",
      "epoch 299 : loss 0.72809994 ; accuracy 0.8374\n",
      "epoch 300 : loss 0.72695404 ; accuracy 0.83753335\n",
      "epoch 301 : loss 0.7258145 ; accuracy 0.8376833\n",
      "epoch 302 : loss 0.7246814 ; accuracy 0.8379167\n",
      "epoch 303 : loss 0.72355455 ; accuracy 0.8380833\n",
      "epoch 304 : loss 0.72243387 ; accuracy 0.83825\n",
      "epoch 305 : loss 0.72131944 ; accuracy 0.83851665\n",
      "epoch 306 : loss 0.72021115 ; accuracy 0.83861667\n",
      "epoch 307 : loss 0.7191089 ; accuracy 0.83875\n",
      "epoch 308 : loss 0.71801263 ; accuracy 0.83885\n",
      "epoch 309 : loss 0.7169224 ; accuracy 0.839\n",
      "epoch 310 : loss 0.71583813 ; accuracy 0.83933336\n",
      "epoch 311 : loss 0.7147597 ; accuracy 0.8396\n",
      "epoch 312 : loss 0.7136871 ; accuracy 0.8398333\n",
      "epoch 313 : loss 0.7126203 ; accuracy 0.8399\n",
      "epoch 314 : loss 0.71155924 ; accuracy 0.8401167\n",
      "epoch 315 : loss 0.71050394 ; accuracy 0.84033334\n",
      "epoch 316 : loss 0.7094542 ; accuracy 0.8405167\n",
      "epoch 317 : loss 0.70841 ; accuracy 0.84071666\n",
      "epoch 318 : loss 0.70737135 ; accuracy 0.8409\n",
      "epoch 319 : loss 0.70633835 ; accuracy 0.84103334\n",
      "epoch 320 : loss 0.7053108 ; accuracy 0.8412\n",
      "epoch 321 : loss 0.70428854 ; accuracy 0.84135\n",
      "epoch 322 : loss 0.70327175 ; accuracy 0.84143335\n",
      "epoch 323 : loss 0.7022603 ; accuracy 0.84153336\n",
      "epoch 324 : loss 0.7012542 ; accuracy 0.84173334\n",
      "epoch 325 : loss 0.7002531 ; accuracy 0.84188336\n",
      "epoch 326 : loss 0.69925743 ; accuracy 0.8419833\n",
      "epoch 327 : loss 0.6982668 ; accuracy 0.84206665\n",
      "epoch 328 : loss 0.69728136 ; accuracy 0.8422167\n",
      "epoch 329 : loss 0.69630104 ; accuracy 0.8423333\n",
      "epoch 330 : loss 0.6953258 ; accuracy 0.84255\n",
      "epoch 331 : loss 0.6943554 ; accuracy 0.8426\n",
      "epoch 332 : loss 0.69339013 ; accuracy 0.8427333\n",
      "epoch 333 : loss 0.6924296 ; accuracy 0.84276664\n",
      "epoch 334 : loss 0.69147414 ; accuracy 0.84288335\n",
      "epoch 335 : loss 0.6905234 ; accuracy 0.8430333\n",
      "epoch 336 : loss 0.6895775 ; accuracy 0.8431\n",
      "epoch 337 : loss 0.6886365 ; accuracy 0.84316665\n",
      "epoch 338 : loss 0.6877001 ; accuracy 0.8433\n",
      "epoch 339 : loss 0.6867685 ; accuracy 0.8434833\n",
      "epoch 340 : loss 0.6858414 ; accuracy 0.84361666\n",
      "epoch 341 : loss 0.684919 ; accuracy 0.8437333\n",
      "epoch 342 : loss 0.6840013 ; accuracy 0.8437833\n",
      "epoch 343 : loss 0.68308806 ; accuracy 0.84395\n",
      "epoch 344 : loss 0.68217945 ; accuracy 0.8441833\n",
      "epoch 345 : loss 0.68127525 ; accuracy 0.84445\n",
      "epoch 346 : loss 0.6803755 ; accuracy 0.84455\n",
      "epoch 347 : loss 0.6794802 ; accuracy 0.84468335\n",
      "epoch 348 : loss 0.67858917 ; accuracy 0.8448833\n",
      "epoch 349 : loss 0.6777026 ; accuracy 0.84495\n",
      "epoch 350 : loss 0.6768204 ; accuracy 0.84508336\n",
      "epoch 351 : loss 0.6759424 ; accuracy 0.84525\n",
      "epoch 352 : loss 0.6750687 ; accuracy 0.84533334\n",
      "epoch 353 : loss 0.6741992 ; accuracy 0.8455167\n",
      "epoch 354 : loss 0.673334 ; accuracy 0.84561664\n",
      "epoch 355 : loss 0.6724729 ; accuracy 0.8458167\n",
      "epoch 356 : loss 0.6716159 ; accuracy 0.84601665\n",
      "epoch 357 : loss 0.670763 ; accuracy 0.8461667\n",
      "epoch 358 : loss 0.66991425 ; accuracy 0.84635\n",
      "epoch 359 : loss 0.6690695 ; accuracy 0.8465\n",
      "epoch 360 : loss 0.6682288 ; accuracy 0.8465667\n",
      "epoch 361 : loss 0.6673921 ; accuracy 0.84678334\n",
      "epoch 362 : loss 0.6665592 ; accuracy 0.84695\n",
      "epoch 363 : loss 0.66573036 ; accuracy 0.84706664\n",
      "epoch 364 : loss 0.6649054 ; accuracy 0.84715\n",
      "epoch 365 : loss 0.6640843 ; accuracy 0.8472667\n",
      "epoch 366 : loss 0.6632671 ; accuracy 0.8473333\n",
      "epoch 367 : loss 0.6624536 ; accuracy 0.84751666\n",
      "epoch 368 : loss 0.6616439 ; accuracy 0.84765\n",
      "epoch 369 : loss 0.660838 ; accuracy 0.8477833\n",
      "epoch 370 : loss 0.66003585 ; accuracy 0.84791666\n",
      "epoch 371 : loss 0.65923744 ; accuracy 0.8480167\n",
      "epoch 372 : loss 0.65844274 ; accuracy 0.84815\n",
      "epoch 373 : loss 0.6576516 ; accuracy 0.84823334\n",
      "epoch 374 : loss 0.6568641 ; accuracy 0.84826666\n",
      "epoch 375 : loss 0.6560802 ; accuracy 0.8483833\n",
      "epoch 376 : loss 0.65529984 ; accuracy 0.84858334\n",
      "epoch 377 : loss 0.6545232 ; accuracy 0.8487167\n",
      "epoch 378 : loss 0.65375 ; accuracy 0.84891665\n",
      "epoch 379 : loss 0.6529802 ; accuracy 0.8490833\n",
      "epoch 380 : loss 0.65221405 ; accuracy 0.8491333\n",
      "epoch 381 : loss 0.6514512 ; accuracy 0.84935\n",
      "epoch 382 : loss 0.6506919 ; accuracy 0.8494167\n",
      "epoch 383 : loss 0.64993596 ; accuracy 0.84943336\n",
      "epoch 384 : loss 0.6491834 ; accuracy 0.84956664\n",
      "epoch 385 : loss 0.6484342 ; accuracy 0.8495833\n",
      "epoch 386 : loss 0.64768827 ; accuracy 0.84971666\n",
      "epoch 387 : loss 0.6469458 ; accuracy 0.8497667\n",
      "epoch 388 : loss 0.6462065 ; accuracy 0.8498333\n",
      "epoch 389 : loss 0.64547056 ; accuracy 0.8498833\n",
      "epoch 390 : loss 0.6447378 ; accuracy 0.8499333\n",
      "epoch 391 : loss 0.6440083 ; accuracy 0.85001665\n",
      "epoch 392 : loss 0.64328194 ; accuracy 0.85013336\n",
      "epoch 393 : loss 0.6425589 ; accuracy 0.8503\n",
      "epoch 394 : loss 0.64183885 ; accuracy 0.8504\n",
      "epoch 395 : loss 0.641122 ; accuracy 0.8505\n",
      "epoch 396 : loss 0.64040834 ; accuracy 0.85066664\n",
      "epoch 397 : loss 0.6396977 ; accuracy 0.85076666\n",
      "epoch 398 : loss 0.6389902 ; accuracy 0.85081667\n",
      "epoch 399 : loss 0.6382857 ; accuracy 0.85085\n",
      "test loss 0.6142092 ; accuracy 0.8577\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = mnist_dataset()\n",
    "for epoch in range(400):\n",
    "    loss, accuracy = train_one_step(model, optimizer, \n",
    "                                    tf.constant(train_data[0], dtype=tf.float32), \n",
    "                                    tf.constant(train_data[1], dtype=tf.int64))\n",
    "    print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n",
    "loss, accuracy = test(model, \n",
    "                      tf.constant(test_data[0], dtype=tf.float32), \n",
    "                      tf.constant(test_data[1], dtype=tf.int64))\n",
    "\n",
    "print('test loss', loss.numpy(), '; accuracy', accuracy.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
